Enterprise Data Platform (EDP) â€“
Comprehensive Product Feature
Document

ğŸ“Œ Overview

The Enterprise Data Platform (EDP) is a robust, configurable, and scalable data ingestion
and orchestration platform built to support the full lifecycle of dataâ€”from ingestion to
reportingâ€”across all business units and data domains within Carlyle.

It allows users to onboard datasets from diverse sources (e.g., CSV, SQL Server, Snowflake,
Oracle, APIs) into the AWS-based data lake, transform them, and expose them for analytics
via interoperable formats such as Apache Iceberg, accessible through Athena, Snowflake,
and other downstream systems.

ğŸ—ï¸ Core Architecture

1. Workflow & Job Orchestration

    â—â€‹ Workflow: A logical container for related ingestion jobs.â€‹

    â—â€‹ Jobs: Individual units responsible for ingesting, transforming, and validating data.â€‹

    â—â€‹ Job Types Supported:â€‹

              â—‹â€‹ File-based (CSV, ZIP, SFTP, etc.)â€‹

              â—‹â€‹ SQL-based (Tables, Queries, Stored Procedures)â€‹

2. Three-Layer Data Architecture

Layer  Description                Storage
                                  Format
Bronz   Raw ingestion layer storing all file versions (e.g., hourly loads).  Parquet
e

Silver  Validated, deduplicated data with transformations (e.g., joins,      Parquet
        lookups).

Gold    Aggregated, reporting-ready datasets with optional direct SQL        Apache
        configuration.                                                       Iceberg

âš™ï¸ Key Features

ğŸ”„ Data Ingestion

    â—â€‹ Supports ingestion from:â€‹
              â—‹â€‹ CSV / Excel filesâ€‹
              â—‹â€‹ APIs (e.g., MailChimp, Sprinklr, Dispatch, SharePoint)â€‹
              â—‹â€‹ RDBMS: SQL Server, Oracle, Postgres, Snowflakeâ€‹

    â—â€‹ Data source registration with auto-schema detection from sample filesâ€‹
    â—â€‹ Configurable file format, naming patterns, header/footer skipping, encoding

         optionsâ€‹

ğŸ“… Scheduling & Execution

    â—â€‹ Manual execution (via play button)â€‹
    â—â€‹ Scheduled runs using AWS EventBridgeâ€‹
    â—â€‹ Supports multiple jobs in a single workflowâ€‹
    â—â€‹ Retry and rerun capabilities from any failed stepâ€‹
ğŸ› ï¸ Data Transformation

    â—â€‹ Column mapping, renaming, and partitioningâ€‹
    â—â€‹ Derived column creation (e.g., uppercase transformation)â€‹
    â—â€‹ Lookup and joins using reference datasets from Silverâ€‹
    â—â€‹ Custom transformation logic supported via Python post-processing scriptsâ€‹

ğŸ§  Direct SQL Mode

    â—â€‹ Allows SQL experts to bypass visual config and define Gold layer logic using custom
         queriesâ€‹

    â—â€‹ Selecting this mode overrides prior configurationsâ€‹

ğŸ” Delta/Incr. Load Support

    â—â€‹ Watermark column-based delta loadsâ€‹
    â—â€‹ Stored Procedure supportâ€‹
    â—â€‹ Parameterized SQL queriesâ€‹
    â—â€‹ Fallback for systems without natural delta fields (e.g., Investran)â€‹

ğŸ§ª Reconciliation & Validation

    â—â€‹ Config-driven reconciliation engineâ€‹
    â—â€‹ Supports multi-file consistency checks (e.g., campaign clicks match across

         reports)â€‹
    â—â€‹ Failed validations logged to CloudWatch, notifications via SNS/emailâ€‹
    â—â€‹ Detailed audit logs stored in S3â€‹

ğŸ” Security & Access Control

    â—â€‹ Scoped visibility via Active Directory groupsâ€‹
    â—â€‹ Four roles supported: Admin, Read-Write, Execute, Read-Onlyâ€‹
    â—â€‹ Access managed per business unit/applicationâ€‹

ğŸ“¡ Integration & Output

    â—â€‹ Apache Iceberg for Gold layer storageâ€‹
    â—â€‹ Accessible from:â€‹

              â—‹â€‹ AWS Athenaâ€‹
              â—‹â€‹ Snowflake (via Glue REST catalog integration)â€‹
              â—‹â€‹ Python clients (boto3)â€‹
    â—â€‹ Compatible with downstream systems like Power BI, PDF/Excel reporting pipelinesâ€‹

ğŸ§© Developer Productivity Features

    â—â€‹ Lambda layers with zipped Python dependencies (under 256MB)â€‹
    â—â€‹ Modular connector structure: pull_from_source() as the entrypointâ€‹
    â—â€‹ Configurations stored in S3, secrets in AWS Secrets Managerâ€‹
    â—â€‹ CDK deployment enabled (CI/CD pipelines to be implemented)â€‹
ğŸ“Š Monitoring & Observability

    â—â€‹ Workflow Monitoring UI:â€‹
              â—‹â€‹ View step-by-step logs for each runâ€‹
              â—‹â€‹ Trace data flow: Source â†’ Bronze â†’ Silver â†’ Goldâ€‹

    â—â€‹ Execution Dashboard:â€‹
              â—‹â€‹ Daily stats on runs, success/failure countsâ€‹
              â—‹â€‹ Error logs, DQ failures, and retry optionsâ€‹

ğŸ§° Supported Technologies

Component  Technology

Orchestration AWS Step Functions

Compute    AWS Lambda

Storage    S3, Iceberg Tables

Querying   Athena, Snowflake

Config Mgmt S3, AWS Secrets Manager

Scheduling AWS EventBridge

Monitoring CloudWatch, S3, SNS
Reporting  Power BI, Custom Python, PDF/Excel

ğŸ›£ï¸ Roadmap & In Progress

    â—â€‹ Reconstruction module (rebuilding datasets from audit history)â€‹
    â—â€‹ Explorer module (Excel-style UI for non-technical users)â€‹
    â—â€‹ Feed module (APIs/files to external systems)â€‹
    â—â€‹ CI/CD pipeline for config deployment

Enterprise Data Platform (EDP) â€“
End-to-End Walkthrough

This walkthrough captures the complete flow of the EDP demo as presented by Pushpendra,
including live configuration steps, Q&A, and explanations around architecture, usage
patterns, and integration capabilities.

1. ğŸ” Introduction: What is EDP?

Pushpendra introduced the Enterprise Data Platform (EDP) as a centralized platform to
ingest, reconcile, transform, and distribute datasets from any source system into the AWS
Data Lake, and beyond.

Core Capabilities:

    â—â€‹ Ingest data from CSV, SQL, Snowflake, Oracle, APIsâ€‹

    â—â€‹ Perform validations and transformationsâ€‹

    â—â€‹ Organize data in Bronze, Silver, and Gold layersâ€‹
    â—â€‹ Feed data to APIs, downstream apps, PDFs, and analytics toolsâ€‹

ğŸ§©2. Workflow Management

ğŸ”¹ Creating a Workflow

    â—â€‹ Navigate to the Workflow Management screen.â€‹
    â—â€‹ Each workflow acts as a container of jobs.â€‹

              â—‹â€‹ Example: A workflow may have 4 jobs (1 per file), executed as one logical
                   unit.â€‹

ğŸ”¹ Metadata Captured:

    â—â€‹ Workflow Name and Descriptionâ€‹
    â—â€‹ Application Name (e.g., ILPA)â€‹
    â—â€‹ Business Unit (e.g., Fund Accounting)â€‹
    â—â€‹ Workflow Type: File vs. Databaseâ€‹
    â—â€‹ Notification Email: For job failures (not for successes)â€‹

Manual or Scheduled Execution:
    â—â€‹ Manual run via UI "Play" buttonâ€‹
    â—â€‹ Scheduled via EventBridgeâ€‹

ğŸ“3. File-Based Job Configuration

ğŸ”¸ Upload & Auto-Schema Detection

    â—â€‹ Upload a sample CSV to generate schema.â€‹
    â—â€‹ Infers column names and data types based on first 100 records.â€‹
ğŸ”¸ Source Configuration Options

    â—â€‹ File sources: S3 (internal/external), SFTPâ€‹
    â—â€‹ Options for:â€‹

              â—‹â€‹ Header/footer row skippingâ€‹
              â—‹â€‹ Encoding (UTF-8, etc.)â€‹
              â—‹â€‹ Regex-based file name patternâ€‹
              â—‹â€‹ Handling ZIP files (automated unzip + processing)â€‹

4. ğŸ—ï¸ Bronze â†’ Silver â†’ Gold Pipeline

Bronze Layer

    â—â€‹ Raw layer with multiple versions of the same data (e.g., hourly trade files).â€‹
    â—â€‹ No transformation; acts as an archive and traceability layer.â€‹

Silver Layer

    â—â€‹ Deduplicated, cleaned version with only one version of the truth.â€‹
    â—â€‹ Users configure:â€‹

              â—‹â€‹ Primary keys, partition keysâ€‹
              â—‹â€‹ Column mappingâ€‹
              â—‹â€‹ Transformations (e.g., UPPERCASE first name)â€‹
              â—‹â€‹ Lookups/Joins (e.g., country code â†’ currency)â€‹

Gold Layer

    â—â€‹ Final reporting layerâ€‹
    â—â€‹ Can use:â€‹
              â—‹â€‹ Visual config (joins, group by, filters)â€‹
              â—‹â€‹ Direct SQL: Overrides visual configs with custom SQLâ€‹

Iceberg tables created in this layer can be accessed by Athena, Snowflake, or other
systems.

5. ğŸ§ª Data Transformation & Lookups

    â—â€‹ Supports creation of derived columnsâ€‹
    â—â€‹ Joins with reference datasets must point to SilverDB tablesâ€‹
    â—â€‹ Allows:â€‹

              â—‹â€‹ Conditional logicâ€‹
              â—‹â€‹ Concatenation, case transformationsâ€‹
              â—‹â€‹ Full SQL override if neededâ€‹

6. ğŸ Custom Post-Processing

Supports custom Python scripts for advanced operations, such as:
    â—â€‹ Generating PDFs from datasetsâ€‹
    â—â€‹ Running pre-defined calculations (e.g., ILPA Excel reports)â€‹

Executed after all jobs complete via post-processing hooks.

7. ğŸ§  Database (SQL) Based Ingestion

Supported Options:

    â—â€‹ Full Table Copyâ€‹
    â—â€‹ Delta Load using watermark columnâ€‹
    â—â€‹ Parameterized SQL Queriesâ€‹
    â—â€‹ Stored Proceduresâ€‹

Supported Databases:
    â—â€‹ SQL Serverâ€‹
    â—â€‹ Oracleâ€‹
    â—â€‹ PostgreSQLâ€‹
    â—â€‹ Snowflakeâ€‹

Secrets are securely stored in AWS Secrets Manager.

   ğŸ’¡ Supports EC2-hosted databases and replica connections as well.

8. âš–ï¸ Delta & Incremental Loading

Delta Support:

    â—â€‹ Uses when_created or last_modified column for tracking changes.â€‹
    â—â€‹ Watermark-based filtering auto-updated per job run.â€‹

Investran Edge Case:

    â—â€‹ Does not have natural delta fields.â€‹
    â—â€‹ Discussed customizing logic using SQL Server change tracking or stored

         procedures.â€‹
    â—â€‹ CDC limitations acknowledged (loss of change history, dependency on Microsoft).â€‹

9. ğŸ§ª Reconciliation & Validation
    â—â€‹ Config-driven reconciliation between files (e.g., MailChimp campaign vs. link
         report)â€‹

    â—â€‹ Example: Compare â€œunique clicksâ€ between two reports.â€‹
    â—â€‹ Failed reconciliations:â€‹

              â—‹â€‹ Logged in CloudWatchâ€‹
              â—‹â€‹ Notifications sent via SNS/emailâ€‹
              â—‹â€‹ Detailed logs also written to S3 audit pathâ€‹

10. ğŸ§­ Monitoring & Logs

Workflow Monitoring

    â—â€‹ View each step:â€‹
              â—‹â€‹ File â†’ Bronze â†’ Silver â†’ Goldâ€‹

    â—â€‹ See:â€‹
              â—‹â€‹ Record countsâ€‹
              â—‹â€‹ Transformation timesâ€‹
              â—‹â€‹ Validation resultsâ€‹
              â—‹â€‹ DQ errorsâ€‹

    â—â€‹ Re-run failed jobs from any stepâ€‹

Execution Dashboard

    â—â€‹ Daily job summariesâ€‹
    â—â€‹ Total runs, failures, successesâ€‹
    â—â€‹ DQ violationsâ€‹
11. ğŸ” Access & Security

    â—â€‹ Access controlled via Active Directory groupsâ€‹
    â—â€‹ 4 levels of access:â€‹

              â—‹â€‹ Adminâ€‹
              â—‹â€‹ Read-Writeâ€‹
              â—‹â€‹ Read-Onlyâ€‹
              â—‹â€‹ Executeâ€‹
    â—â€‹ Users only see workflows/jobs in their business unitâ€‹

12. ğŸ“Š Data Access & Reporting

    â—â€‹ Athena: SQL access to Iceberg tablesâ€‹
    â—â€‹ Power BI: Direct connectivity via Snowflake or Athenaâ€‹
    â—â€‹ Python: Use boto3 client to query Iceberg tablesâ€‹
    â—â€‹ Snowflake: Glue REST catalog integration exposes Icebergâ€‹

Future additions:
    â—â€‹ Excel-like data browser UI for non-technical usersâ€‹
    â—â€‹ Data feed module to push to external systemsâ€‹

ğŸš§ 13. Additional Notes & Q&A Highlights

ğŸ”¹ Custom PDF Reporting

    â—â€‹ Stivan discussed existing .NET app generating PDFs from SQL Serverâ€‹
    â—â€‹ Plan: Replace SQL Server backend with Iceberg or Snowflake queriesâ€‹
    â—â€‹ Possible to connect C# app via Python/Lambda to trigger reportsâ€‹

ğŸ”¹ Stored Procedure Support

    â—â€‹ Demo included configuring a stored proc as a jobâ€‹
    â—â€‹ Used to simulate data pre-processing from SQL-based sourcesâ€‹

ğŸ”¹ Incremental Data Challenges

    â—â€‹ Investran lacks timestampsâ€‹
    â—â€‹ Change tracking used in SQL Serverâ€‹
    â—â€‹ Option to mirror logic in EDP's stored proc supportâ€‹

âœ… Summary

The EDP Platform provides an end-to-end, modular, and fully orchestrated pipeline for:
    â—â€‹ Multi-source ingestionâ€‹
    â—â€‹ Schema inferenceâ€‹
    â—â€‹ Validation and reconciliationâ€‹
    â—â€‹ Transformation and joinsâ€‹
    â—â€‹ Monitoring, alerting, and loggingâ€‹
    â—â€‹ Flexible consumption (Athena, Snowflake, Power BI, Lambda)â€‹

The system was designed with interoperability, modularity, and observability as key
pillars, ensuring business users and technical teams can operate confidently at scale.
